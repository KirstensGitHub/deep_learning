{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1 Parameter Norm Penalties"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Im most deep learning scenarios, the best fitting model (lowest error) is a larae model with regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For neural nets, we use a parameter norm penalty the penalizes only the weights of the affiine transformatioin at each layer... Weights control hoow variables interact, whereas the biases control only a single variable (we don't lose out too much by not regularizing bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Regularizing weights can also lead to underfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "w --> weights that are regularized <br/>\n",
    "theta --> all params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In some cases, you might want a different penalty (and alpha coefficient) at eaach layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## L^2 Parameter Regularization (Weight Decay)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "AKA \"ridge regression\" or Tikhonov regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This regularization strategy drives the weights closer to the origin1by adding a regularization term Ω(θ) = 1/2*||w||^2 to the objective function."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"images/L2.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will further simplify the analysis by making a quadratic approximationto the objective function in the neighborhood of the value of the weights thatobtains minimal unregularized training cost,w*=arg min_w J(w)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If the objectivefunction is truly quadratic, as in the case of ﬁtting a linear regression model with mean squared error, then the approximation is perfect. The approximation of Jisgiven by:\n",
    "\n",
    "<b>J(θ) = J(w∗) + (1/2)*(w −w∗).T * H(w − w∗)</b>\n",
    "\n",
    "where H is the Hessian matrix of J with respect to w evaluated at w∗. There is no ﬁrst-order term in this quadratic approximation, because w∗ is deﬁned to be a minimum, where the gradient vanishes. Likewise, because w∗is the location of aminimum of J, we can conclude that H is positive semideﬁnite. The minimum of J occurs where its gradient \n",
    "\n",
    "<b>∇wˆJ(w) = H(w − w∗)  </b>\n",
    "\n",
    "is equal to zero"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Page 228 --> if you do eigenvector decomposition, you see that weight decay rescales w* along the axes defined by the eigenvectors of H.\n",
    "\n",
    "For directions where eigenvalues of H are big, regularization has small effect. Directions where eigenvectors are small will be shrunk to almost zero magnitude.\n",
    "\n",
    "<b> Only directions along which the parameters contribute signiﬁcantly to reducingthe objective function are preserved relatively intact </b>\n",
    "\n",
    "In directions that do notcontribute to reducing the objective function, a small eigenvalue of the Hessian tells us that movement in this direction will not signiﬁcantly increase the gradient. Components of the weight vector corresponding to such unimportant directionsare decayed away through the use of the regularization throughout training"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"images/L2_fig.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example of L2 in linear regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"images/L2_lin_reg.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 7.1.2 L1 Regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ω(θ) = ||w||1= SUM |w_i|, "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ sum of abs values of individual parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
