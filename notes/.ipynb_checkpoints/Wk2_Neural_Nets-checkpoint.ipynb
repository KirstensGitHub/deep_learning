{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chapter 6"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Deep Feedforward Networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### aka feedforward neural nets, or multilayer perceptrons (MLP's)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "data --> computatios --> output\n",
    "\n",
    "No feedback connections (feedback connections --> \"recurrent\" neural nets)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Layers\n",
    "\n",
    "The output layer has a \"goal\" as defined by the data; given x, the output layer should produce output as close to y as possible.\n",
    "\n",
    "The other layers do not have specific, desired output based on the data. Rather, the algorithm decides how best to use them. As such, they are called \"hidden layers\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Neural nets and linear models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Advantages of linear models:\n",
    "    \n",
    "- can be fit efficiently, reliably\n",
    "    - closed form or convex optimization\n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Use linear models for non-linear data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use a transformation (phi) to do a non-linear mapping of x, and use linear model on the transformed data.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What phi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- very generic \n",
    "    - (like the infinite-dimensional one kernel machines use based on RBF kernel)\n",
    "    - PRO: will always fit training data\n",
    "    - CON: poor generalization\n",
    "    \n",
    "- manually engineer phi\n",
    "    - PRO: specific, draws on professionals' specialized knowledge\n",
    "    - CON: time intensive, old school <br />\n",
    "    \n",
    "    \n",
    "\n",
    "- <b>learn phi (deep learning approach)\n",
    "    - y = f(x;θ, w) = φ(x;θ).T * w\n",
    "    - phi serves as a hidden layer </b>\n",
    "    \n",
    "        - CON: lose convexity of the training problem\n",
    "    \n",
    "        - <b> PROS: INCLUDES THE PROS OF THE PREVIOUS TWO! </b>\n",
    "            - can be generic --> use broad family of phi equations\n",
    "            - can use domain specific knowledge --> create phi families expected to do well \n",
    "    \n",
    "    \n",
    "    \n",
    "\n",
    "* what's RBF? : https://en.wikipedia.org/wiki/Radial_basis_function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example: Learn XOR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "XOR function:\n",
    "\n",
    "    inputs: x1, x2\n",
    "\n",
    "    outputs: if exactly one input (x1 OR x2) = 1 --> 1\n",
    "    \n",
    "             else --> 0\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loss function\n",
    "\n",
    "J(θ) =1/4 * SUM ( x∈X(f∗(x) − f(x; θ))^2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Mapping x onto a new space\n",
    "\n",
    "The XOR problem cannot be solved by a linear function at first. We have to remap it in a way that makes it linear-friendly.\n",
    "\n",
    "Common: use an affine transformation (controlled by learned paramters) followed by a fixed non-linear activation function (like ReLu)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "<center><img style=\"display: inline\" src=\"images/6_1.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Here is the feedforward network to solve XOR problem, drawn two ways:\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"images/6_2.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ReLu activation function\n",
    "\n",
    "\n",
    "<b>Relu basically just turns any negative inputs to zero</b>\n",
    "\n",
    "returns the if input > 0 and 0 if the input < 0 \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Come back to the istantiation of this network on pages 171 - 172 if needed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Gradient Based Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "non-linear neural networks --> most loss functions non-convex "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "most neural nets trained by iterative gradient-based optimizers that drive cost function very low "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>For feedforward neural networks, important to initialize all weights to small random values.</b> \n",
    "The biases may be initialized to zero or to small positive values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cost Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Often use max likelihood --> cost function is cross-entropy between predictions & training data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we take a simpler approach --> predict some statistic of y conditioned on x. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Total cost function often combines primary cost function w/ regularization term"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Learning Conditional Distributions with Maximum Likelihood\n",
    "\n",
    "<b> From textbook:</b>\n",
    "\n",
    "An advantage of this approach of deriving the cost function from maximumlikelihood is that it removes the burden of designing cost functions for each model. Specifying a modelp(y | x) automatically determines a cost functionlog p(y | x).\n",
    "\n",
    "One recurring theme throughout neural network design is that the gradient ofthe cost function must be large and predictable enough to serve as a good guidefor the learning algorithm. Functions that saturate (become very ﬂat) underminethis objective because they make the gradient become very small. In many casesthis happens because the activation functions used to produce the output of thehidden units or the output units saturate. The negative log-likelihood helps toavoid this problem for many models. Several output units involve anexpfunctionthat can saturate when its argument is very negative. Thelogfunction in thenegative log-likelihood cost function undoes theexpof some output units. We willdiscuss the interaction between the cost function and the choice of output unit insection 6.2.2.\n",
    "\n",
    "One unusual property of the cross-entropy cost used to perform maximumlikelihood estimation is that it usually does not have a minimum value when appliedto the models commonly used in practice. For discrete output variables, mostmodels are parametrized in such a way that they cannot represent a probabilityof zero or one, but can come arbitrarily close to doing so. Logistic regressionis an example of such a model. For real-valued output variables, if the modelcan control the density of the output distribution (for example, by learning thevariance parameter of a Gaussian output distribution) then it becomes possibleto assign extremely high density to the correct training set outputs, resulting incross-entropy approaching negative inﬁnity. Regularization techniques describedin chapter 7 provide several diﬀerent ways of modifying the learning problem sothat the model cannot reap unlimited reward in this way"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Learning Conditional Statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Sometimes, we might just want to learn one conditional statistic of y given x.\n",
    "\n",
    "For example, we might just want to predict the mean of y."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Calculus of variations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Can calculate loss function for predicting the mean and median of y (via calculus of variations).\n",
    "\n",
    "(for reference, look up \"mean absolute error\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Cross-entropy cost function more popular than MSE or mean abs error</b>...\n",
    "\n",
    "...output units that saturate may produce very small gradients when combined w/ MSE, etc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Output Units\n",
    "\n",
    "Not much new info in this section (p 177)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Linear Units for Gaussian Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"Because linear units do not saturate, they pose little diﬃculty for gradient-based optimization algorithms and may be used with a wide variety of optimizationalgorithms.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid Units for Bernoulli Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "mostly skipped this section, defines logit\n",
    "\n",
    "WIKIPEDIA:\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"images/logit.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TEXTBOOK (p 179):\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"images/book_logit.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "MORE:\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"images/add.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "saturates only when (1-2y)z is very negative, i.e. when the model already has the right answer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for extremely incorrect z, softplus doesn't shrink the gradient at all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: In software implementations,to avoid numerical problems, it is best to write the negative log-likelihood as afunction of z, rather than as a function ofˆy=σ(z). If the sigmoid function under ﬂows to zero, then taking the logarithm of ˆy yields negative inﬁnity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Softmax Units for Multinoulli Output Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "softmax can be used any time we want too represent a probability distribution over a discrete variable with n possible values\n",
    "\n",
    "can be considered a generalization of the sigmoid function used to represent probability distribution over a binary variable"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"images/softmax.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "sigmoid can saturate --> when input is extremely negative or extremely positive\n",
    "\n",
    "softmax can saturate --> differences between input values becomes extreme\n",
    "        \n",
    "        --> causes many cost functions to saturate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NOTE: because all softmax outputs must sum to 1, it mimics \"lateral inhibition\". \n",
    "\n",
    "in extreme cases (one especially large value), it can become a winner-takes-all"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"soft\" max is continuous and differentiable <br/>\n",
    "(softened version of argmax)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Ouput types\n",
    "\n",
    "Most common output units:\n",
    "Linear, Sigmoid, Softmax\n",
    "\n",
    "For info on others, see pages 184 - 187"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Hidden Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### How to choose type of hidden units?\n",
    "\n",
    "Issue unique to feed forward neural networks"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Most common...\n",
    "\n",
    "Relu is popular and nobody seems to know what is best and why... \n",
    "\n",
    "Mostly trial and error..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rectified Linear Units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Rectified linear units use the activation function:</b>\n",
    "    \n",
    "g(z) = max{0,z}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Typically used on top of an affine transformation:</b>\n",
    "    \n",
    "h = g(W^(T)x+b)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^ note: start b elements at small positive values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Drawback: cannot learn via gradient based methods when their activation is zero</b>\n",
    "\n",
    "can use generalizationsof relu to guarantee they receive a gradient everywhere\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b> Generalizations</b>: abs value rectification, leaky ReLU, parametric ReLU (PReLU), Maxout units"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "for details on generalizations, see pages 189-190"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Logistic Sigmoid and Hyberbolic Tangent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"images/logistic_sig.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Hidden Units (p192-193)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Radial Basis Function, Softplus, Hard tanh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Architectural Design"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Main considerations:</b>\n",
    "    \n",
    "Depth of network (# of layers)\n",
    "\n",
    "Width of each layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Neural Nets can do lots!</b>\n",
    "\n",
    "..any continuous function on a closed and bounded subset of R^n is Borel measurable and therefore may be approximated by a neural network. \n",
    "\n",
    "A neural network mayalso approximate any function mapping from any ﬁnite dimensional discrete space to another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Universal approximation theorem:</b> there exists a network large enough to achieve whatever accuracy we desire, but we don't know how big that network will be"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "FOR DETAILS ON CHOOSING SIZE OF NEURAL NET, SEE PAGES 195-197"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Other Architectural Considerations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "layers don't have to be organized in a chain\n",
    "\n",
    "pairs of layers can be connected in different ways\n",
    "- not every node from this layer has to feed into every node of the next layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Back-Propagation and Other Differentiation Algorithms"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feed forward networks...\n",
    "\n",
    "[1] <b>Forward propagation :</b>\n",
    "    input propagates forward through each layer to cost function\n",
    "    \n",
    "[2] <b>Back-propagation :</b>\n",
    "    info from cost function flows back through layers to compute gradient\n",
    "    \n",
    "[3] <b>Gradient descent :</b>\n",
    "    use the gradient to perform learning (e.g. via stochastic gradient descent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Typically, the gradient we want is the gradient of the cost function with rrerspect to the params, theta."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Chain rule of calculus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"images/chain_rule.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
