{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1 Reference Material\n",
    "\n",
    "5.5 & 5.7.1 in https://www.deeplearningbook.org/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5 - Maximum Likelihood Estimation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Q: How can we derive functions that will be good estimators?\n",
    "\n",
    "A: Use maximum likelihood principle!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Basic Idea\n",
    "Estimate the parameters of a probability distribution so that your model predicts that the most likely data is the data you have (observed)\n",
    "\n",
    "### Max likelihood estimate \n",
    "Max likelihood estimate is the point in parameter space that maximizes likelihood function\n",
    "\n",
    "### How to find it...\n",
    "If likelihood function is differentiable, can use derivative test to find maxima\n",
    "\n",
    "For linear regression model, can use ordinary least squares estimator "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference text\n",
    "<br />\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"mle_1.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "^^ Find the probability of obtaining each datapoint in your sample (right of pi symbol), and multiply all of those probabilities together (pi symbol) to get probability of obtaining your observed sample. \n",
    "\n",
    "Maximize this probability (argmax) so your model assesses your observed data as the most likely data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Log likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Multiplying a bunch of probabilities can be inconvenient \n",
    "<br /><br /> (e.g. when stuff like this crops up https://en.wikipedia.org/wiki/Arithmetic_underflow; KZ: also issues of concavity / convexity of likelihood equation for gradient descent?)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Luckily, argmax is impervious to some changes:\n",
    "\n",
    "It does not change if we take the log of the likelihood, which lets us sum instead of multiplying, since log(a * b) = log(a) + log(b) \n",
    "\n",
    "<center><img style=\"display: inline\" src=\"mle_2.png\" alt=\"MLE\" width=\"800\"> </center>\n",
    "\n",
    "It does not change if we scale the cost function, so we can divide the whole thing by m.\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"mle_33.png\" alt=\"MLE\" width=\"800\"> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### KZ's understanding: <br />\n",
    "<b> Divide by m --> instead of getting the sum of a bunch of log probabilities, you get their (weighted?) average..</b>\n",
    "\n",
    "higher probability (p) --> log(p) ~= negative values approaching zero \n",
    "<br />lower probability (p) --> log(p) ~= negative values approaching negative infinity \n",
    "\n",
    "<b> KZ: So, to maximize probabilities, you want the \"expected\" (E) average log probability to be closest to zero? (in this case, argmax, bc all values are negative or zero?) </b>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Ways to find Max Likelihood Estimate \n",
    "\n",
    "Maximize the log likelihood / expectation equation (above), or minimize KL divergence (below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reference text: <br />\n",
    "\n",
    "<center><img style=\"display: inline\" src=\"mle_4.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### \"Big picture\" of KL divergence:\n",
    "\n",
    "Consider two probability distributions: your data (P) and your model (Q).\n",
    "\n",
    "<b>Kullback–Leibler divergence is the average difference in the number of bits required for encoding samples of P using a code optimized for Q versus a code optimized for P. </b>\n",
    "\n",
    "KL divergence is essentially the same as \"cross entropy\"; minimizing one minimizes the other."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Why minimize KL divergence..\n",
    "...instead of minimizing \"Negative Log Likelihood\" (NLL) or maximizing log likelihood?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Because...\n",
    "\n",
    "<br/><b> KL divergence has a known minimum of 0</b>.<br/> Negative Log Likelihood (NLL) can be negative.\n",
    "\n",
    "KZ: how / when is NLL negative?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.1 - Conditional Log Likelihood & Mean Squared Error\n",
    "\n",
    "You can also use Max Likelihood Estimation to do linear regression (using \"conditional log likelihood\")!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"mle_5.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You can generalize maximum likelihood to make predictions about y given x. <br/>Just optmize your paramaters to maximize the likelihood of y given the model with input x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### High level"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of producing a single prediction (y_hat) we can now think of linear regression as producing a conditional distribution p(y|x).\n",
    "\n",
    "With an infinitely large training set, we might get cases where the same inputted x yields different values of y. The prob distribution tries to fit all of these as best as possible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Typical linear regression approach"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If you play with the conditional log likelihood equations, you can convert this approach to more familiar linear regression and mean squared error equations (it's the same thing)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"mle_6.png\" alt=\"MLE\" width=\"800\"> </center>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.5.2 Properties of Maximum Likelihood\n",
    "\n",
    "So, why use Maximum Likelihood Estimation for linear regression if you can use simpler equations? Because MLE has some advantageous properties.."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Key takeaways:\n",
    "MLE offers <b> consistency</b> and <b> efficiency</b>, making it a commonly preferred estimator for machine learning.  \n",
    "\n",
    "When the number of datapoints is small, risking overfitting, regularization strategies (like \"weight decay\") can provide a biased version of maximum likelihood -- one with has less variance when training data is limited"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Note: I only did a very cursory skim of this section!\n",
    "\n",
    "For details on MLE properties, and requirements for MLE consistency, see pages 132-133.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 5.7.1 Probabilistic Supervised Learning\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><img style=\"display: inline\" src=\"mle_7.png\" alt=\"MLE\" width=\"800\"> </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Easy to generalize linear regression (conditional log likelihood) to do supervised classification by calculating the probability of a particular class (just need probability for one class, if only two-classes exist)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Sigmoid\n",
    "\n",
    "The normal distribution we typically use for linear regression is paramaterized in terms of a mean. A distribution over a binary variable is tricker because it will always be between 0 and 1.\n",
    "\n",
    "To convert the output of linear function to be on the interval between 0 and 1, use the logistic sigmoid function:\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "p(y = 1 | x; θ) = σ(θ^T x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ^This is what we call logistic regression"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
